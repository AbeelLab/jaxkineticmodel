{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b16dfc76-0c78-416c-9565-c0825087a06a",
   "metadata": {},
   "source": [
    "# Neural ODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8768437-4934-45d8-9d34-d0783e81cf94",
   "metadata": {},
   "source": [
    "This example trains a [Neural ODE](https://arxiv.org/abs/1806.07366) to reproduce a toy dataset of nonlinear oscillators.\n",
    "\n",
    "This example is available as a Jupyter notebook [here](https://github.com/patrick-kidger/diffrax/blob/main/examples/neural_ode.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "276cbbe5-dac1-4814-807c-e50cc633b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import diffrax\n",
    "import equinox as eqx  # https://github.com/patrick-kidger/equinox\n",
    "import jax\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import optax  # https://github.com/deepmind/optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f36a4f-3813-4f84-8d61-96500ea1f237",
   "metadata": {},
   "source": [
    "We use [Equinox](https://github.com/patrick-kidger/equinox) to build neural networks. We use [Optax](https://github.com/deepmind/optax) for optimisers (Adam etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5016bd6c-4981-4783-a3cf-087b89b680c4",
   "metadata": {},
   "source": [
    "Recalling that a neural ODE is defined as\n",
    "\n",
    "$y(t) = y(0) + \\int_0^t f_\\theta(s, y(s)) ds$,\n",
    "\n",
    "then here we're now about to define the $f_\\theta$ that appears on that right hand side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "243412cd-9f19-489f-a10e-bf0eb8bf3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Func(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "\n",
    "    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.mlp = eqx.nn.MLP(\n",
    "            in_size=data_size,\n",
    "            out_size=data_size,\n",
    "            width_size=width_size,\n",
    "            depth=depth,\n",
    "            activation=jnn.softplus,\n",
    "            key=key,\n",
    "        )\n",
    "\n",
    "    def __call__(self, t, y, args):\n",
    "        return self.mlp(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa17137-a851-4214-9187-53711d0e07da",
   "metadata": {},
   "source": [
    "Here we wrap up the entire ODE solve into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb14ae0-1aa5-4e10-ba3b-d977d5d6ac3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralODE(eqx.Module):\n",
    "    func: Func\n",
    "\n",
    "    def __init__(self, data_size, width_size, depth, *, key, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.func = Func(data_size, width_size, depth, key=key)\n",
    "\n",
    "    def __call__(self, ts, y0):\n",
    "        solution = diffrax.diffeqsolve(\n",
    "            diffrax.ODETerm(self.func),\n",
    "            diffrax.Tsit5(),\n",
    "            t0=ts[0],\n",
    "            t1=ts[-1],\n",
    "            dt0=ts[1] - ts[0],\n",
    "            y0=y0,\n",
    "            stepsize_controller=diffrax.PIDController(rtol=1e-3, atol=1e-6),\n",
    "            saveat=diffrax.SaveAt(ts=ts),\n",
    "        )\n",
    "        return solution.ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d3264c-2edc-465e-b61f-05125645e1df",
   "metadata": {},
   "source": [
    "Toy dataset of nonlinear oscillators. Sample paths look like deformed sines and cosines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29225b09-3a50-4a7f-bbcf-5824d44f3e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_data(ts, *, key):\n",
    "    y0 = jr.uniform(key, (2,), minval=-0.6, maxval=1)\n",
    "\n",
    "    def f(t, y, args):\n",
    "        x = y / (1 + y)\n",
    "        return jnp.stack([x[1], -x[0]], axis=-1)\n",
    "\n",
    "    solver = diffrax.Tsit5()\n",
    "    dt0 = 0.1\n",
    "    saveat = diffrax.SaveAt(ts=ts)\n",
    "    sol = diffrax.diffeqsolve(\n",
    "        diffrax.ODETerm(f), solver, ts[0], ts[-1], dt0, y0, saveat=saveat\n",
    "    )\n",
    "    ys = sol.ys\n",
    "    return ys\n",
    "\n",
    "\n",
    "def get_data(dataset_size, *, key):\n",
    "    ts = jnp.linspace(0, 10, 100)\n",
    "    key = jr.split(key, dataset_size)\n",
    "    ys = jax.vmap(lambda key: _get_data(ts, key=key))(key)\n",
    "    return ts, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6505387d-6900-401d-9ceb-b741f349f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    assert all(array.shape[0] == dataset_size for array in arrays)\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jr.permutation(key, indices)\n",
    "        (key,) = jr.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        print(end,dataset_size)\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd937f6-4642-4b8a-b309-aae63a9269dc",
   "metadata": {},
   "source": [
    "Main entry point. Try runnning `main()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "135540f6-d5ea-4c79-b083-0b86fd3edbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_size=256,\n",
    "batch_size=32,\n",
    "lr_strategy=(3e-3, 3e-3),\n",
    "steps_strategy=(100, 200),\n",
    "length_strategy=(0.1, 1),\n",
    "width_size=64,\n",
    "depth=2,\n",
    "seed=5678,\n",
    "plot=False,\n",
    "# print_every=100,\n",
    "\n",
    "key = jr.PRNGKey(seed)\n",
    "data_key, model_key, loader_key = jr.split(key, 3)\n",
    "\n",
    "ts, ys = get_data(dataset_size, key=data_key)\n",
    "_, length_size, data_size = ys.shape\n",
    "\n",
    "model = NeuralODE(data_size, width_size, depth, key=model_key)\n",
    "\n",
    "# Training loop like normal.\n",
    "#\n",
    "# Only thing to notice is that up until step 500 we train on only the first 10% of\n",
    "# each time series. This is a standard trick to avoid getting caught in a local\n",
    "# minimum.\n",
    "\n",
    "@eqx.filter_value_and_grad\n",
    "def grad_loss(model, ti, yi):\n",
    "    y_pred = jax.vmap(model, in_axes=(None, 0))(ti, yi[:, 0])\n",
    "    return jnp.mean((yi - y_pred) ** 2)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(ti, yi, model, opt_state):\n",
    "    loss, grads = grad_loss(model, ti, yi)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return loss, model, opt_state\n",
    "\n",
    "for lr, steps, length in zip(lr_strategy, steps_strategy, length_strategy):\n",
    "    optim = optax.adabelief(lr)\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "    _ts = ts[: int(length_size * length)]\n",
    "    _ys = ys[:, : int(length_size * length)]\n",
    "    for step, (yi,) in zip(\n",
    "        range(steps), dataloader((_ys,), batch_size, key=loader_key)\n",
    "    ):\n",
    "        start = time.time()\n",
    "        loss, model, opt_state = make_step(_ts, yi, model, opt_state)\n",
    "        end = time.time()\n",
    "        if (step % print_every) == 0 or step == steps - 1:\n",
    "            print(f\"Step: {step}, Loss: {loss}, Computation time: {end - start}\")\n",
    "\n",
    "if plot:\n",
    "    plt.plot(ts, ys[0, :, 0], c=\"dodgerblue\", label=\"Real\")\n",
    "    plt.plot(ts, ys[0, :, 1], c=\"dodgerblue\")\n",
    "    model_y = model(ts, ys[0, 0])\n",
    "    plt.plot(ts, model_y[:, 0], c=\"crimson\", label=\"Model\")\n",
    "    plt.plot(ts, model_y[:, 1], c=\"crimson\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"neural_ode.png\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fba5156d-22d4-40c9-991b-e6b29a53abed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 256\n",
      "NeuralODE(\n",
      "  func=Func(\n",
      "    mlp=MLP(\n",
      "      layers=(\n",
      "        Linear(\n",
      "          weight=f32[64,2],\n",
      "          bias=f32[64],\n",
      "          in_features=2,\n",
      "          out_features=64,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        Linear(\n",
      "          weight=f32[64,64],\n",
      "          bias=f32[64],\n",
      "          in_features=64,\n",
      "          out_features=64,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        Linear(\n",
      "          weight=f32[2,64],\n",
      "          bias=f32[2],\n",
      "          in_features=64,\n",
      "          out_features=2,\n",
      "          use_bias=True\n",
      "        )\n",
      "      ),\n",
      "      activation=None,\n",
      "      final_activation=None,\n",
      "      use_bias=True,\n",
      "      use_final_bias=True,\n",
      "      in_size=2,\n",
      "      out_size=2,\n",
      "      width_size=64,\n",
      "      depth=2\n",
      "    )\n",
      "  )\n",
      ")\n",
      "NeuralODE(\n",
      "  func=Func(\n",
      "    mlp=MLP(\n",
      "      layers=(\n",
      "        Linear(\n",
      "          weight=f32[64,2],\n",
      "          bias=f32[64],\n",
      "          in_features=2,\n",
      "          out_features=64,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        Linear(\n",
      "          weight=f32[64,64],\n",
      "          bias=f32[64],\n",
      "          in_features=64,\n",
      "          out_features=64,\n",
      "          use_bias=True\n",
      "        ),\n",
      "        Linear(\n",
      "          weight=f32[2,64],\n",
      "          bias=f32[2],\n",
      "          in_features=64,\n",
      "          out_features=2,\n",
      "          use_bias=True\n",
      "        )\n",
      "      ),\n",
      "      activation=None,\n",
      "      final_activation=None,\n",
      "      use_bias=True,\n",
      "      use_final_bias=True,\n",
      "      in_size=2,\n",
      "      out_size=2,\n",
      "      width_size=64,\n",
      "      depth=2\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "seed=1\n",
    "\n",
    "dataset_size=256,\n",
    "batch_size=1\n",
    "lr_strategy=(3e-3, 3e-3),\n",
    "steps_strategy=(100, 200),\n",
    "length_strategy=(0.1, 1),\n",
    "width_size=64,\n",
    "depth=2,\n",
    "length=20\n",
    "\n",
    "dataset_size=256\n",
    "key = jr.PRNGKey(seed)\n",
    "data_key, model_key, loader_key = jr.split(key, 3)\n",
    "\n",
    "ts, ys = get_data(dataset_size, key=data_key)\n",
    "_, length_size, data_size = ys.shape\n",
    "\n",
    "optim=optax.adabelief(learning_rate=1e-3)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "_ts = ts[: int(length_size * length)]\n",
    "_ys = ys[:, : int(length_size * length)]\n",
    "# print(ys.shape)\n",
    "\n",
    "\n",
    "@eqx.filter_value_and_grad\n",
    "def grad_loss(model, ti, yi):\n",
    "    y_pred = jax.vmap(model, in_axes=(None, 0))(ti, yi[:, 0])\n",
    "    return jnp.mean((yi - y_pred) ** 2)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(ti, yi, model, opt_state):\n",
    "    loss, grads = grad_loss(model, ti, yi)\n",
    "    updates, opt_state = optim.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return loss, model, opt_state\n",
    "\n",
    "\n",
    "for step, (yi,) in zip(range(steps), dataloader((_ys,), batch_size, key=loader_key)):\n",
    "    # print(yi)\n",
    "    \n",
    "    loss, model, opt_state = make_step(_ts, yi, model, opt_state)\n",
    "    loss,grads=grad_loss(model,_ts,yi)\n",
    "    print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3bb6f21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function filter_jit in module equinox._jit:\n",
      "\n",
      "filter_jit(fun=<object object at 0x7aabedbdb7a0>, *, donate: Literal['all', 'all-except-first', 'warn', 'warn-except-first', 'none'] = 'none', **jitkwargs)\n",
      "    An easier-to-use version of `jax.jit`. All JAX and NumPy arrays are traced, and\n",
      "    all other types are held static.\n",
      "\n",
      "    **Arguments:**\n",
      "\n",
      "    - `fun` is a pure function to JIT compile.\n",
      "    - `donate` indicates whether the buffers of JAX arrays are donated or not. It\n",
      "        should either be:\n",
      "        - `'all'`: donate all arrays and suppress all warnings about unused buffers;\n",
      "        - `'all-except-first'`: donate all arrays except for those in the first\n",
      "            argument, and suppress all warnings about unused buffers;\n",
      "        - `'warn'`: as above, but don't suppress unused buffer warnings;\n",
      "        - `'warn-except-first'`: as above, but don't suppress unused buffer warnings;\n",
      "        - `'none'`: no buffer donation. (This the default.)\n",
      "\n",
      "    **Returns:**\n",
      "\n",
      "    The JIT'd version of `fun`.\n",
      "\n",
      "    !!! example\n",
      "\n",
      "        ```python\n",
      "        # Basic behaviour\n",
      "        @eqx.filter_jit\n",
      "        def f(x, y):  # both args traced if arrays, static if non-arrays\n",
      "            return x + y, x - y\n",
      "\n",
      "        f(jnp.array(1), jnp.array(2))  # both args traced\n",
      "        f(jnp.array(1), 2)  # first arg traced, second arg static\n",
      "        f(1, 2)  # both args static\n",
      "        ```\n",
      "\n",
      "    !!! info\n",
      "\n",
      "        Donating arguments allows their underlying memory to be used in the\n",
      "        computation. This can produce speed and memory improvements, but means that you\n",
      "        cannot use any donated arguments again, as their underlying memory has been\n",
      "        overwritten. (JAX will throw an error if you try to.)\n",
      "\n",
      "    !!! info\n",
      "\n",
      "        If you want to trace Python `bool`/`int`/`float`/`complex` as well then you\n",
      "        can do this by wrapping them into a JAX array: `jnp.asarray(x)`.\n",
      "\n",
      "        If you want to donate only some arguments then this can be done by setting\n",
      "        `filter_jit(donate=\"all-except-first\")` and then passing all arguments that you\n",
      "        don't want to donate through the first argument. (Packing multiple values into\n",
      "        a tuple if necessary.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(eqx.filter_jit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "421ab97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function is_inexact_array in module equinox._filters:\n",
      "\n",
      "is_inexact_array(element: Any) -> bool\n",
      "    Returns `True` if `element` is an inexact (i.e. floating or complex) JAX/NumPy\n",
      "    array.\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea7c0626-7e83-4a2b-84c9-44bcfc147f35",
   "metadata": {},
   "source": [
    "Some notes on speed:\n",
    "The hyperparameters for the above example haven't really been optimised. Try experimenting with them to see how much faster you can make this example run. There's lots of things you can try tweaking:\n",
    "\n",
    "- The size of the neural network.\n",
    "- The numerical solver.\n",
    "- The step size controller, including both its step size and its tolerances.\n",
    "- The length of the dataset. (Do you really need to use all of a time series every time?)\n",
    "- Batch size, learning rate, choice of optimiser.\n",
    "- ... etc.!\n",
    "\n",
    "Some notes on being Markov:\n",
    "\n",
    "- This example has assumed that the problem is Markov. Essentially, that the data `ys` is a complete observation of the system, and that we're not missing any channels. Note how the result of our model is evolving in data space. This is unlike e.g. an RNN, which has hidden state, and a linear map from hidden state to data.\n",
    "- If we wanted we could generalise this to the non-Markov case: inside `NeuralODE`, project the initial condition into some high-dimensional latent space, do the ODE solve there, then take a linear map to get the output. See the [Latent ODE example](../latent_ode) for an example doing this as part of a generative model; also see [Augmented Neural ODEs](https://arxiv.org/abs/1904.01681) for a short paper on it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
