{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/plent/anaconda3/envs/neural_odes/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch  # This is all you need to use both PyTorch and TorchScript!\n",
    "print(torch.__version__)\n",
    "import torchviz\n",
    "torch.manual_seed(191009)  # set the seed for reproducibility\n",
    "from torchviz import make_dot, make_dot_from_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.8219, 0.8990, 0.6670, 0.8277],\n",
      "        [0.5176, 0.4017, 0.8545, 0.7336],\n",
      "        [0.6013, 0.6992, 0.2618, 0.6668]]), tensor([[0.8219, 0.8990, 0.6670, 0.8277],\n",
      "        [0.5176, 0.4017, 0.8545, 0.7336],\n",
      "        [0.6013, 0.6992, 0.2618, 0.6668]]))\n",
      "MyCell(\n",
      "  (linear): Linear(in_features=4, out_features=4, bias=True)\n",
      ")\n",
      "(tensor([[ 0.8573,  0.6190,  0.5774,  0.7869],\n",
      "        [ 0.3326,  0.0530,  0.0702,  0.8114],\n",
      "        [ 0.7818, -0.0506,  0.4039,  0.7967]], grad_fn=<TanhBackward0>), tensor([[ 0.8573,  0.6190,  0.5774,  0.7869],\n",
      "        [ 0.3326,  0.0530,  0.0702,  0.8114],\n",
      "        [ 0.7818, -0.0506,  0.4039,  0.7967]], grad_fn=<TanhBackward0>))\n"
     ]
    }
   ],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(x + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell()\n",
    "x = torch.rand(3, 4)\n",
    "h = torch.rand(3, 4)\n",
    "print(my_cell(x, h))\n",
    "\n",
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell()\n",
    "print(my_cell)\n",
    "print(my_cell(x, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short, TorchScript provides tools to capture the definition of your model, even in light of the flexible and dynamic nature of PyTorch. Let’s begin by examining what we call tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyCell(\n",
      "  original_name=MyCell\n",
      "  (linear): Linear(original_name=Linear)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0416,  0.7165,  0.5299, -0.0434],\n",
       "         [ 0.2687,  0.1412,  0.6382,  0.1054],\n",
       "         [ 0.3480,  0.5014,  0.6016, -0.3498]], grad_fn=<TanhBackward0>),\n",
       " tensor([[-0.0416,  0.7165,  0.5299, -0.0434],\n",
       "         [ 0.2687,  0.1412,  0.6382,  0.1054],\n",
       "         [ 0.3480,  0.5014,  0.6016, -0.3498]], grad_fn=<TanhBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.linear(x) + h)\n",
    "        return new_h, new_h\n",
    "    \n",
    "\n",
    "\n",
    "my_cell = MyCell()\n",
    "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "print(traced_cell)\n",
    "traced_cell(x, h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[-0.0416,  0.7165,  0.5299, -0.0434],\n",
      "        [ 0.2687,  0.1412,  0.6382,  0.1054],\n",
      "        [ 0.3480,  0.5014,  0.6016, -0.3498]], grad_fn=<TanhBackward0>), tensor([[-0.0416,  0.7165,  0.5299, -0.0434],\n",
      "        [ 0.2687,  0.1412,  0.6382,  0.1054],\n",
      "        [ 0.3480,  0.5014,  0.6016, -0.3498]], grad_fn=<TanhBackward0>))\n",
      "(tensor([[-0.0416,  0.7165,  0.5299, -0.0434],\n",
      "        [ 0.2687,  0.1412,  0.6382,  0.1054],\n",
      "        [ 0.3480,  0.5014,  0.6016, -0.3498]], grad_fn=<TanhBackward0>), tensor([[-0.0416,  0.7165,  0.5299, -0.0434],\n",
      "        [ 0.2687,  0.1412,  0.6382,  0.1054],\n",
      "        [ 0.3480,  0.5014,  0.6016, -0.3498]], grad_fn=<TanhBackward0>))\n"
     ]
    }
   ],
   "source": [
    "print(my_cell(x, h))\n",
    "print(traced_cell(x, h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def forward(self,\n",
      "    argument_1: Tensor) -> Tensor:\n",
      "  return torch.neg(argument_1)\n",
      "\n",
      "def forward(self,\n",
      "    x: Tensor,\n",
      "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
      "  dg = self.dg\n",
      "  linear = self.linear\n",
      "  _0 = torch.add((dg).forward((linear).forward(x, ), ), h)\n",
      "  _1 = torch.tanh(_0)\n",
      "  return (_1, _1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62410/1812836291.py:3: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if x.sum() > 0:\n"
     ]
    }
   ],
   "source": [
    "class MyDecisionGate(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        if x.sum() > 0:\n",
    "            return x\n",
    "        else:\n",
    "            return -x\n",
    "\n",
    "class MyCell(torch.nn.Module):\n",
    "    def __init__(self, dg):\n",
    "        super(MyCell, self).__init__()\n",
    "        self.dg = dg\n",
    "        self.linear = torch.nn.Linear(4, 4)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n",
    "        return new_h, new_h\n",
    "\n",
    "my_cell = MyCell(MyDecisionGate())\n",
    "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
    "\n",
    "print(traced_cell.dg.code)\n",
    "print(traced_cell.code)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying this on mechanisms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([2.], requires_grad=True), Parameter containing:\n",
      "tensor([3.], requires_grad=True)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.8000], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the following two can be merged potentially:\n",
    "class Torch_Irrev_MM_Uni(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 vmax: float,\n",
    "                 km_substrate: float,\n",
    "                 to_be_learned):\n",
    "        super(Torch_Irrev_MM_Uni, self).__init__()\n",
    "\n",
    "        if to_be_learned[0]:\n",
    "            # make mu a learnable parameter\n",
    "            self.vmax = torch.nn.Parameter(torch.tensor([vmax]))\n",
    "        else:\n",
    "            self.vmax = vmax\n",
    "\n",
    "        if to_be_learned[1]:\n",
    "            self.km_substrate = torch.nn.Parameter(torch.Tensor([km_substrate]))\n",
    "        else:\n",
    "            self.km_substrate = km_substrate\n",
    "\n",
    "    def forward(self, substrate):\n",
    "        nominator = (self.vmax)*(substrate/self.km_substrate)\n",
    "        # nominator=self.vmax*substrate\n",
    "        # denominator=self.km_substrate + substrate\n",
    "        denominator = (1+(substrate/self.km_substrate))\n",
    "        return nominator/denominator\n",
    "    \n",
    "\n",
    "substrate=torch.Tensor([2.1])\n",
    "mechanism=Torch_Irrev_MM_Uni(vmax=2.0,km_substrate=3.0,to_be_learned=[True,True])\n",
    "traced_mechanism=torch.jit.trace(mechanism,(substrate))\n",
    "\n",
    "print(list(traced_mechanism.parameters()))\n",
    "traced_mechanism.forward(2)\n",
    "\n",
    "\n",
    "# mechanism(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.2 µs ± 6.66 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "mechanism(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.1 µs ± 1.31 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "traced_mechanism(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following two can be merged potentially:\n",
    "class Torch_Irrev_MM_Uni(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 vmax: float,\n",
    "                 km_substrate: float,\n",
    "                 to_be_learned):\n",
    "        super(Torch_Irrev_MM_Uni, self).__init__()\n",
    "\n",
    "        if to_be_learned[0]:\n",
    "            # make mu a learnable parameter\n",
    "            self.vmax = torch.nn.Parameter(torch.tensor([vmax]))\n",
    "        else:\n",
    "            self.vmax = vmax\n",
    "\n",
    "        if to_be_learned[1]:\n",
    "            self.km_substrate = torch.nn.Parameter(torch.Tensor([km_substrate]))\n",
    "        else:\n",
    "            self.km_substrate = km_substrate\n",
    "\n",
    "    def calculate(self, substrate):\n",
    "        nominator = (self.vmax)*(substrate/self.km_substrate)\n",
    "        # nominator=self.vmax*substrate\n",
    "        # denominator=self.km_substrate + substrate\n",
    "        denominator = (1+(substrate/self.km_substrate))\n",
    "        return nominator/denominator\n",
    "    \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skimpy-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
