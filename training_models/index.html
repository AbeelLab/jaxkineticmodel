<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Training models - jaxkineticmodel documentation</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Training models";
        var mkdocs_page_input_path = "training_models.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> jaxkineticmodel documentation
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../building_models/">Building and simulating models</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../SBML/">Loading SBML models</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Training models</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#the-trainer-object">The Trainer object</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#setting-up-the-trainer-object-training">Setting up the trainer object + training</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#additional-rounds">Additional rounds</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#trainer-configurability">Trainer configurability</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#optimization-in-logarithmic-or-linear-space">Optimization in logarithmic or linear space</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#optimizer-choices">Optimizer choices</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#customize-loss-functions">Customize loss functions</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#future-configuration-options">Future configuration options</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#references">References</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../simulated-DBTL/">Simulated-DBTL</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../glycolysis/">Custom models</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">jaxkineticmodel documentation</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Training models</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="training-metabolic-kinetic-models">Training metabolic kinetic models</h1>
<p>Here, we showcase an parameter optimization process with simulated data<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>.</p>
<h2 id="the-trainer-object">The <code>Trainer</code> object</h2>
<p>The <code>Trainer</code> object requires a few inputs. First, it requires a <code>SBMLModel</code> or a <code>NeuralODEBuild</code> object to be used. The second input is a datasets to fit on. Here, we show the fitting of a previously reported Serine Biosynthesis model<sup id="fnref2:1"><a class="footnote-ref" href="#fn:1">1</a></sup>.</p>
<h4 id="setting-up-the-trainer-object-training">Setting up the trainer object + training</h4>
<p>First, we load the necessary functions </p>
<div class="codehilite"><pre><span></span><code><span class="kn">from</span> <span class="nn">jaxkineticmodel.parameter_estimation.training</span> <span class="kn">import</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">jaxkineticmodel.load_sbml.sbml_model</span> <span class="kn">import</span> <span class="n">SBMLModel</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">import</span> <span class="nn">optax</span>
</code></pre></div>

<p>Then, we load the model, data and initialize the trainer object. </p>
<div class="codehilite"><pre><span></span><code><span class="c1"># load model</span>
<span class="n">model_name</span> <span class="o">=</span> <span class="s2">&quot;Smallbone2013_SerineBiosynthesis&quot;</span>
<span class="n">filepath</span> <span class="o">=</span> <span class="s2">&quot;models/sbml_models/working_models/&quot;</span> <span class="o">+</span> <span class="n">model_name</span> <span class="o">+</span> <span class="s2">&quot;.xml&quot;</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SBMLModel</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
<span class="n">kinetic_model</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">get_kinetic_model</span><span class="p">()</span>

<span class="c1"># load data</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;datasets/Smallbone2013 - Serine biosynthesis/Smallbone2013 - Serine biosynthesis_dataset.csv&quot;</span><span class="p">,</span>
                      <span class="n">index_col</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1">#initialize the trainer object. The required inputs are model and data. We will do 300 iterations of gradient descent</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">kinetic_model</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>

<span class="c1"># latin hypercube</span>
</code></pre></div>

<p>We next perform a latin hypercube sampling for a certain initial guess, with lower and upperbound defined with respect to these values. We want five initializations (normally this should be higher).</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># latin hypercube</span>
<span class="n">base_parameters</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">parameters</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">parameters</span><span class="p">))))</span>
<span class="n">parameter_sets</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">latinhypercube_sampling</span><span class="p">(</span><span class="n">base_parameters</span><span class="p">,</span>
                                                 <span class="n">lower_bound</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">10</span><span class="p">,</span>
                                                 <span class="n">upper_bound</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                                                 <span class="n">N</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</code></pre></div>

<p>To initiate training, you simply call the function <code>Trainer.train()</code></p>
<div class="codehilite"><pre><span></span><code><span class="n">optimized_parameters</span><span class="p">,</span> <span class="n">loss_per_iteration</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">loss_per_iteration</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Iterations&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Log Loss&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="loss" src="../images/loss_per_iter.png" /></p>
<p><span style="font-size: 0.8em;"><b>Figure 1:</b> Loss per iteration for five initializations.</span></p>
<h4 id="additional-rounds">Additional rounds</h4>
<p>Suppose the fit is not to your liking, or we first want to do a pre-optimization of a large set of parameters and then filter promising sets, 
one can continue the optimization by re-running the <code>trainer</code> object with the set of optimized parameters.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># next round</span>
<span class="n">params_round1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">optimized_parameters</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">parameter_sets</span> <span class="o">=</span> <span class="n">params_round1</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">optimized_parameters2</span><span class="p">,</span> <span class="n">loss_per_iteration2</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>

<span class="c1"># plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">loss_per_iteration</span><span class="p">[</span><span class="n">i</span><span class="p">]),</span> <span class="n">loss_per_iteration2</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>

<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Iterations&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Log Loss&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_yscale</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div>

<p><img alt="loss_extended" src="../images/loss_per_iter_extended.png" /></p>
<p><span style="font-size: 0.8em;"><b>Figure 2:</b> Loss per iteration for five initializations, extended with 500 rounds of
gradient descent.</span></p>
<h2 id="trainer-configurability">Trainer configurability</h2>
<h3 id="optimization-in-logarithmic-or-linear-space">Optimization in logarithmic or linear space</h3>
<p>Optimization in logarithmic space has shown to work well for systems biology models<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup> and is implemented as the default. 
To change to using gradient descent in a linear parameter space, you can restart the <code>Trainer</code> object. 
When the loss function is not specified (see below), a mean squared error loss is used.</p>
<div class="codehilite"><pre><span></span><code><span class="c1"># log or linear space example</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">kinetic_model</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">optim_space</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">)</span>
</code></pre></div>

<h3 id="optimizer-choices">Optimizer choices</h3>
<p>Jaxkineticmodel is compatible with optimizers from <a href="https://optax.readthedocs.io/en/latest/">optax</a>. To use these, simply
pass the optimizer to the <code>Trainer</code> object (with required arguments). </p>
<div class="codehilite"><pre><span></span><code><span class="c1"># optimizer change with optax optimizers</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">kinetic_model</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
</code></pre></div>

<h3 id="customize-loss-functions">Customize loss functions</h3>
<p>Jaxkinetic model uses as a default a log-transformed parameter space and a mean centered loss function. However, users
may want to present their own custom loss. </p>
<div class="codehilite"><pre><span></span><code><span class="c1"># own loss function</span>
<span class="k">def</span> <span class="nf">log_mean_centered_loss_func2</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">ts</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">to_include</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;log_mean_centered_loss_func with index of state variables on which</span>
<span class="sd">    to train on. For example in the case of incomplete knowledge of the system&quot;&quot;&quot;</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp2</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="o">~</span><span class="n">jnp</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">ys</span><span class="p">))</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">ys</span><span class="p">)</span>
    <span class="n">y0</span> <span class="o">=</span> <span class="n">ys</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">ts</span><span class="p">,</span> <span class="n">y0</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="n">ys</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">y_pred</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">scale</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">ys</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="n">ys</span> <span class="o">/=</span> <span class="n">scale</span>
    <span class="n">y_pred</span> <span class="o">/=</span> <span class="n">scale</span>

    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">ys</span><span class="p">[:,</span> <span class="n">to_include</span><span class="p">]</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_pred</span><span class="p">[:,</span> <span class="n">to_include</span><span class="p">]</span>
    <span class="n">non_nan_count</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y_pred</span> <span class="o">-</span> <span class="n">ys</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">non_nan_count</span>
    <span class="k">return</span> <span class="n">loss</span>


<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">kinetic_model</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">dataset</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">300</span><span class="p">)</span>
<span class="n">trainer</span><span class="o">.</span><span class="n">_create_loss_func</span><span class="p">(</span><span class="n">log_mean_centered_loss_func2</span><span class="p">,</span> <span class="n">to_include</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>  <span class="c1">#only include specimen 1 in the dataset</span>
</code></pre></div>

<p>The loss function has mandatory arguments <code>params</code>,<code>ts</code>,<code>ys</code>. All other required arguments (e.g., <code>to_include</code>) are 
passed to <code>trainer._create_loss_func()</code></p>
<p>NOTE: the use of custom loss function can depend on whether you perform your optimization in log-space or not. If 
you want to perform a custom loss function in log-space, you need to exponentiate your parameters within 
the loss function.</p>
<h3 id="future-configuration-options">Future configuration options</h3>
<p>We aim to further add configurability of the adjoint option from Diffrax.  </p>
<h2 id="references">References</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Smallbone, K., &amp; Stanford, N. J. (2013). Kinetic modeling of metabolic pathways: Application to serine biosynthesis. Systems Metabolic Engineering: Methods and Protocols, 113-121.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Villaverde, A. F., Fr√∂hlich, F., Weindl, D., Hasenauer, J., &amp; Banga, J. R. (2019). Benchmarking optimization methods for parameter estimation in large kinetic models. Bioinformatics, 35(5), 830-838.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
</ol>
</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../SBML/" class="btn btn-neutral float-left" title="Loading SBML models"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../simulated-DBTL/" class="btn btn-neutral float-right" title="Simulated-DBTL">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../SBML/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../simulated-DBTL/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
